{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import get_full_repo_name, Repository, notebook_login, create_repo\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, pipeline, Trainer, TrainingArguments, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "TEXT_FOLDER = './data/CADEC.v2/cadec/text/'\n",
    "OG_ANN_FOLDER = './data/CADEC.v2/cadec/original/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------------- FUNCTIONS ------------------- #\n",
    "#####################################################\n",
    "\n",
    "# Function to read text from a .txt file\n",
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to read annotations from a .ann file\n",
    "def read_annotations(file_path):\n",
    "    annotations = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            # Get only the important information\n",
    "            information = parts[1].split()\n",
    "\n",
    "            # Get values from info\n",
    "            label = information[0]\n",
    "            # Semicolon problem not solved\n",
    "            start_values = [value for value in information[1].split(';')]\n",
    "            end_values = [value for value in information[2].split(';')]\n",
    "\n",
    "            for start, end in zip(start_values, end_values):\n",
    "                annotations.append({\"start\": int(start), \"end\": int(end), \"label\": label})\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Update tags based on the annotations\n",
    "def annotate_text(doc, annotations):\n",
    "    tags = [\"O\"] * len(doc)\n",
    "    for annotation in annotations:\n",
    "        start, end, label = annotation[\"start\"], annotation[\"end\"], annotation[\"label\"]\n",
    "        start_token = None\n",
    "        for i, token in enumerate(doc):\n",
    "            if start_token is None and token.idx >= start:\n",
    "                start_token = i\n",
    "            if token.idx + len(token) >= end and start_token is not None:\n",
    "                for j in range(start_token, i + 1):\n",
    "                    if j == start_token:\n",
    "                        tags[j] = f\"B-{label}\"\n",
    "                    else:\n",
    "                        tags[j] = f\"I-{label}\"\n",
    "                break\n",
    "    return tags\n",
    "\n",
    "# Function that aligns tokens with labels\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else label2id[labels[word_id]]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as the previous token\n",
    "            label = label2id[labels[word_id]]\n",
    "            # If the label is B-XXX, we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "# Function that tokenizees and aligns the labels with the tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# count total amount each entity type is in the training set\n",
    "def count_entities(dataset):\n",
    "    entity_count = {}\n",
    "    for doc in dataset:\n",
    "        for tag in doc['ner_tags']:\n",
    "            if tag in entity_count:\n",
    "                entity_count[tag] += 1\n",
    "            else:\n",
    "                entity_count[tag] = 1\n",
    "    return entity_count\n",
    "\n",
    "# Compute metrics for the training evaluation\n",
    "# def compute_metrics(eval_preds):\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "#     # Remove ignored index (special tokens) and convert to labels\n",
    "#     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "#     true_predictions = [\n",
    "#         [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": all_metrics[\"overall_precision\"],\n",
    "#         \"recall\": all_metrics[\"overall_recall\"],\n",
    "#         \"f1\": all_metrics[\"overall_f1\"],\n",
    "#         \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "#     }\n",
    "\n",
    "# Improved compute metrics for evaluation during training\n",
    "def compute_metrics(eval_preds):\n",
    "# def improved_compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    entity_metrics = {entity: {\"precision\": 0, \"recall\": 0, \"f1\": 0} for entity in label_names}\n",
    "\n",
    "    for entity in label_names:\n",
    "        #  precision, recall, and F1 per the entity type\n",
    "        true_entity_labels = [1 if entity in labels else 0 for labels in true_labels]\n",
    "        predicted_entity_labels = [1 if entity in labels else 0 for labels in true_predictions]\n",
    "\n",
    "        entity_metrics[entity][\"precision\"] = precision_score(true_entity_labels, predicted_entity_labels)\n",
    "        entity_metrics[entity][\"recall\"] = recall_score(true_entity_labels, predicted_entity_labels)\n",
    "        entity_metrics[entity][\"f1\"] = f1_score(true_entity_labels, predicted_entity_labels)\n",
    "\n",
    "    # flatten, list of lists to list\n",
    "    flat_true_labels = [label for labels in true_labels for label in labels]\n",
    "    flat_true_predictions = [label for labels in true_predictions for label in labels]\n",
    "\n",
    "    # macro and micro average F1 scores\n",
    "    macro_f1 = np.mean([entity_metrics[entity][\"f1\"] for entity in label_names])\n",
    "    micro_f1 = f1_score(flat_true_labels, flat_true_predictions, average='micro')\n",
    "\n",
    "    all_metrics = {\n",
    "        \"entity_metrics\": entity_metrics,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "    }\n",
    "\n",
    "    return all_metrics\n",
    "\n",
    "# Function that pos process the predictions and returns true labels and true preds\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert annotations into IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "TEXT_FOLDER = './data/CADEC.v2/cadec/text/'\n",
    "OG_ANN_FOLDER = './data/CADEC.v2/cadec/original/'\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get a list of all text files in the folder\n",
    "text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "\n",
    "# Initialize lists to store data\n",
    "data_arr = []\n",
    "\n",
    "# Loop through each text file\n",
    "for text_file in text_files:\n",
    "    # print(text_file)\n",
    "    # Build file paths\n",
    "    txt_file_path = os.path.join(TEXT_FOLDER, text_file)\n",
    "    ann_file_path = os.path.join(OG_ANN_FOLDER, text_file.replace(\".txt\", \".ann\"))\n",
    "\n",
    "    # Read text from the .txt file\n",
    "    text = read_text(txt_file_path)\n",
    "\n",
    "    # Read annotations from the .ann file\n",
    "    annotations = read_annotations(ann_file_path)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Perform the annotation loop\n",
    "    tags_array = annotate_text(doc, annotations)\n",
    "\n",
    "    # Create a word by word array\n",
    "    words_array = [token.text for token in doc]\n",
    "\n",
    "    # Store the data for this document\n",
    "    data_arr.append({\"tokens\": words_array, \"ner_tags\": tags_array})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas dataframe\n",
    "df = pd.DataFrame(data_arr)\n",
    "\n",
    "# Print the dataframe\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data into desired HuggingFace format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# -- Pandas DataFrame to HuggingFace DatasetDict -- #\n",
    "#####################################################\n",
    "\n",
    "# Split data into train, val & test sets\n",
    "data, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Convert to dataset dictionaries\n",
    "train_dataframe = pd.DataFrame({\n",
    "    'id': train.index.values.astype(str),\n",
    "    'tokens': train.tokens.values,\n",
    "    'ner_tags': train.ner_tags.values\n",
    "})\n",
    "dev_dataframe = pd.DataFrame({\n",
    "    'id': val.index.values.astype(str),\n",
    "    'tokens': val.tokens.values,\n",
    "    'ner_tags': val.ner_tags.values\n",
    "})\n",
    "test_dataframe = pd.DataFrame({\n",
    "    'id': test.index.values.astype(str),\n",
    "    'tokens': test.tokens.values,\n",
    "    'ner_tags': test.ner_tags.values\n",
    "})\n",
    "\n",
    "# From dictionaries to DatasetDict\n",
    "train_dataset = datasets.Dataset.from_dict(train_dataframe)\n",
    "dev_dataset = datasets.Dataset.from_dict(dev_dataframe)\n",
    "test_dataset = datasets.Dataset.from_dict(test_dataframe)\n",
    "\n",
    "# Join into one\n",
    "raw_data = datasets.DatasetDict({'train': train_dataset, 'validation': dev_dataset, 'test': test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------ Statistics from dataset ------------ #\n",
    "#####################################################\n",
    "\n",
    "train_count = count_entities(raw_data['train'])\n",
    "entity_count = count_entities(raw_data['train'])\n",
    "entity_count\n",
    "\n",
    "# calculate for val and test set and then sum\n",
    "entity_count_val = count_entities(raw_data['validation'])\n",
    "entity_count_test = count_entities(raw_data['test'])\n",
    "\n",
    "for key in entity_count_val:\n",
    "    entity_count[key] += (entity_count_val[key] + entity_count_test[key])\n",
    "    \n",
    "print ( \"all datasets: \\n\", entity_count) \n",
    "print ( \"train dataset: \\n\", train_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ----------------- Get tokenizer ----------------- #\n",
    "#####################################################\n",
    "\n",
    "# Get tokenizer from checkpint\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Set labels with Id\n",
    "label_names = ['O', 'B-ADR', 'I-ADR',\n",
    "               'B-Disease', 'I-Disease',\n",
    "               'B-Drug', 'I-Drug',\n",
    "               'B-Finding', 'I-Finding',\n",
    "               'B-Symptom', 'I-Symptom']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "tokenized_datasets = raw_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_data[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------------- Get Model ------------------- #\n",
    "#####################################################\n",
    "\n",
    "# Get data collector with the previously defined tokenizer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Get metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Get model from checkpoint\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Number of labels the model has\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------- Train Base-line Model ------------- #\n",
    "#####################################################\n",
    "\n",
    "# Set training arguments\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Push to hub (saving the train in HuggingFace)\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing and Training with Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------ Set-up with Accelerator ------------ #\n",
    "#####################################################\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = 'bert-finetuned-ner-accelerate/'\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Get dataloader for train and evaluation\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "# Get model from the new pretrained checkpoint\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Set optimizer AdamW in this case\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Prepare\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "# Set accelerator train parameters\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# --------------- HuggingFace Setup --------------- #\n",
    "#####################################################\n",
    "\n",
    "# Set model and repository name\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "print(f'Repo name: {repo_name}')\n",
    "\n",
    "# Create repository\n",
    "create_repo(\"Gorgoura/bert-finetuned-ner-accelerate\", repo_type=\"model\")\n",
    "\n",
    "# Set ouput directory and make the repository\n",
    "output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ----------- Training with Accelerator ----------- #\n",
    "#####################################################\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------ Evaluating the training ------------ #\n",
    "#####################################################\n",
    "\n",
    "# Initializing the trainer again, to run the evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    # eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results and the checkpoint used\n",
    "print (f\"checkpoint: {model_checkpoint}\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"{key}: {value:.3f}\")\n",
    "\n",
    "# Print all entity metrics\n",
    "entity_metrics = test_results[\"eval_entity_metrics\"]\n",
    "macro_f1 = test_results[\"eval_macro_f1\"]\n",
    "micro_f1 = test_results[\"eval_micro_f1\"]\n",
    "\n",
    "# Display the evaluation results\n",
    "print(\"Entity Metrics:\")\n",
    "for entity, metrics in entity_metrics.items():\n",
    "    print(f\"{entity}: Precision={metrics['precision']}, Recall={metrics['recall']}, F1={metrics['f1']}\")\n",
    "\n",
    "print(f\"Macro-Averaged F1: {macro_f1}\")\n",
    "print(f\"Micro-Averaged F1: {micro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
