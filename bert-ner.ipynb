{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import get_full_repo_name, Repository, notebook_login, create_repo\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, pipeline, Trainer, TrainingArguments, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "# from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "TEXT_FOLDER = './data/CADEC.v2/cadec/text/'\n",
    "OG_ANN_FOLDER = './data/CADEC.v2/cadec/original/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------------- FUNCTIONS ------------------- #\n",
    "#####################################################\n",
    "\n",
    "# Function to read text from a .txt file\n",
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to read annotations from a .ann file\n",
    "def read_annotations(file_path):\n",
    "    annotations = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            # Get only the important information\n",
    "            information = parts[1].split()\n",
    "\n",
    "            # Get values from info\n",
    "            label = information[0]\n",
    "            # Semicolon problem not solved\n",
    "            start_values = [value for value in information[1].split(';')]\n",
    "            end_values = [value for value in information[2].split(';')]\n",
    "\n",
    "            for start, end in zip(start_values, end_values):\n",
    "                annotations.append({\"start\": int(start), \"end\": int(end), \"label\": label})\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Update tags based on the annotations\n",
    "def annotate_text(doc, annotations):\n",
    "    tags = [\"O\"] * len(doc)\n",
    "    for annotation in annotations:\n",
    "        start, end, label = annotation[\"start\"], annotation[\"end\"], annotation[\"label\"]\n",
    "        start_token = None\n",
    "        for i, token in enumerate(doc):\n",
    "            if start_token is None and token.idx >= start:\n",
    "                start_token = i\n",
    "            if token.idx + len(token) >= end and start_token is not None:\n",
    "                for j in range(start_token, i + 1):\n",
    "                    if j == start_token:\n",
    "                        tags[j] = f\"B-{label}\"\n",
    "                    else:\n",
    "                        tags[j] = f\"I-{label}\"\n",
    "                break\n",
    "    return tags\n",
    "\n",
    "# Function that aligns tokens with labels\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else label2id[labels[word_id]]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as the previous token\n",
    "            label = label2id[labels[word_id]]\n",
    "            # If the label is B-XXX, we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "# Function that tokenizees and aligns the labels with the tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# count total amount each entity type is in the training set\n",
    "def count_entities(dataset):\n",
    "    entity_count = {}\n",
    "    for doc in dataset:\n",
    "        for tag in doc['ner_tags']:\n",
    "            if tag in entity_count:\n",
    "                entity_count[tag] += 1\n",
    "            else:\n",
    "                entity_count[tag] = 1\n",
    "    return entity_count\n",
    "\n",
    "# Compute metrics for the training evaluation\n",
    "# def compute_metrics(eval_preds):\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "#     # Remove ignored index (special tokens) and convert to labels\n",
    "#     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "#     true_predictions = [\n",
    "#         [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": all_metrics[\"overall_precision\"],\n",
    "#         \"recall\": all_metrics[\"overall_recall\"],\n",
    "#         \"f1\": all_metrics[\"overall_f1\"],\n",
    "#         \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "#     }\n",
    "\n",
    "# Improved compute metrics for evaluation during training\n",
    "def compute_metrics(eval_preds):\n",
    "# def improved_compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    entity_metrics = {entity: {\"precision\": 0, \"recall\": 0, \"f1\": 0} for entity in label_names}\n",
    "\n",
    "    for entity in label_names:\n",
    "        #  precision, recall, and F1 per the entity type\n",
    "        true_entity_labels = [1 if entity in labels else 0 for labels in true_labels]\n",
    "        predicted_entity_labels = [1 if entity in labels else 0 for labels in true_predictions]\n",
    "\n",
    "        entity_metrics[entity][\"precision\"] = precision_score(true_entity_labels, predicted_entity_labels)\n",
    "        entity_metrics[entity][\"recall\"] = recall_score(true_entity_labels, predicted_entity_labels)\n",
    "        entity_metrics[entity][\"f1\"] = f1_score(true_entity_labels, predicted_entity_labels)\n",
    "\n",
    "    # flatten, list of lists to list\n",
    "    flat_true_labels = [label for labels in true_labels for label in labels]\n",
    "    flat_true_predictions = [label for labels in true_predictions for label in labels]\n",
    "\n",
    "    # macro and micro average F1 scores\n",
    "    macro_f1 = np.mean([entity_metrics[entity][\"f1\"] for entity in label_names])\n",
    "    micro_f1 = f1_score(flat_true_labels, flat_true_predictions, average='micro')\n",
    "\n",
    "    all_metrics = {\n",
    "        \"entity_metrics\": entity_metrics,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "    }\n",
    "\n",
    "    return all_metrics\n",
    "\n",
    "# Function that pos process the predictions and returns true labels and true preds\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert annotations into IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "TEXT_FOLDER = './data/CADEC.v2/cadec/text/'\n",
    "OG_ANN_FOLDER = './data/CADEC.v2/cadec/original/'\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get a list of all text files in the folder\n",
    "text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "\n",
    "# Initialize lists to store data\n",
    "data_arr = []\n",
    "\n",
    "# Loop through each text file\n",
    "for text_file in text_files:\n",
    "    # print(text_file)\n",
    "    # Build file paths\n",
    "    txt_file_path = os.path.join(TEXT_FOLDER, text_file)\n",
    "    ann_file_path = os.path.join(OG_ANN_FOLDER, text_file.replace(\".txt\", \".ann\"))\n",
    "\n",
    "    # Read text from the .txt file\n",
    "    text = read_text(txt_file_path)\n",
    "\n",
    "    # Read annotations from the .ann file\n",
    "    annotations = read_annotations(ann_file_path)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Perform the annotation loop\n",
    "    tags_array = annotate_text(doc, annotations)\n",
    "\n",
    "    # Create a word by word array\n",
    "    words_array = [token.text for token in doc]\n",
    "\n",
    "    # Store the data for this document\n",
    "    data_arr.append({\"tokens\": words_array, \"ner_tags\": tags_array})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas dataframe\n",
    "df = pd.DataFrame(data_arr)\n",
    "\n",
    "# Print the dataframe\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data into desired HuggingFace format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# -- Pandas DataFrame to HuggingFace DatasetDict -- #\n",
    "#####################################################\n",
    "\n",
    "# Split data into train, val & test sets\n",
    "data, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Convert to dataset dictionaries\n",
    "train_dataframe = pd.DataFrame({\n",
    "    'id': train.index.values.astype(str),\n",
    "    'tokens': train.tokens.values,\n",
    "    'ner_tags': train.ner_tags.values\n",
    "})\n",
    "dev_dataframe = pd.DataFrame({\n",
    "    'id': val.index.values.astype(str),\n",
    "    'tokens': val.tokens.values,\n",
    "    'ner_tags': val.ner_tags.values\n",
    "})\n",
    "test_dataframe = pd.DataFrame({\n",
    "    'id': test.index.values.astype(str),\n",
    "    'tokens': test.tokens.values,\n",
    "    'ner_tags': test.ner_tags.values\n",
    "})\n",
    "\n",
    "# From dictionaries to DatasetDict\n",
    "train_dataset = datasets.Dataset.from_dict(train_dataframe)\n",
    "dev_dataset = datasets.Dataset.from_dict(dev_dataframe)\n",
    "test_dataset = datasets.Dataset.from_dict(test_dataframe)\n",
    "\n",
    "# Join into one\n",
    "raw_data = datasets.DatasetDict({'train': train_dataset, 'validation': dev_dataset, 'test': test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all datasets: \n",
      " {'B-ADR': 5952, 'I-ADR': 8669, 'O': 109813, 'B-Disease': 282, 'I-Disease': 191, 'B-Drug': 1799, 'B-Finding': 425, 'I-Finding': 394, 'I-Drug': 226, 'B-Symptom': 269, 'I-Symptom': 261}\n",
      "train dataset: \n",
      " {'B-ADR': 3800, 'I-ADR': 5570, 'O': 69098, 'B-Disease': 184, 'I-Disease': 123, 'B-Drug': 1116, 'B-Finding': 244, 'I-Finding': 231, 'I-Drug': 144, 'B-Symptom': 175, 'I-Symptom': 166}\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# ------------ Statistics from dataset ------------ #\n",
    "#####################################################\n",
    "\n",
    "train_count = count_entities(raw_data['train'])\n",
    "entity_count = count_entities(raw_data['train'])\n",
    "entity_count\n",
    "\n",
    "# calculate for val and test set and then sum\n",
    "entity_count_val = count_entities(raw_data['validation'])\n",
    "entity_count_test = count_entities(raw_data['test'])\n",
    "\n",
    "for key in entity_count_val:\n",
    "    entity_count[key] += (entity_count_val[key] + entity_count_test[key])\n",
    "    \n",
    "print ( \"all datasets: \\n\", entity_count) \n",
    "print ( \"train dataset: \\n\", train_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb06c102feb418eb146d0e91d3fc3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0937d9ea13594cfb8f5d33bd16ec8daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1ce9191e954093bdae3bda0a3e6e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################################################\n",
    "# ----------------- Get tokenizer ----------------- #\n",
    "#####################################################\n",
    "\n",
    "# Get tokenizer from checkpint\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Set labels with Id\n",
    "label_names = ['O', 'B-ADR', 'I-ADR',\n",
    "               'B-Disease', 'I-Disease',\n",
    "               'B-Drug', 'I-Drug',\n",
    "               'B-Finding', 'I-Finding',\n",
    "               'B-Symptom', 'I-Symptom']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "tokenized_datasets = raw_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_data[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "# ------------------- Get Model ------------------- #\n",
    "#####################################################\n",
    "\n",
    "# Get data collector with the previously defined tokenizer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Get metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Get model from checkpoint\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Number of labels the model has\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0ad42434b64891bc3450e5996e6aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbb534ef465440498922f33a43e5ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9653179190751445, 'recall': 0.9597701149425287, 'f1': 0.962536023054755}, 'I-ADR': {'precision': 0.9608938547486033, 'recall': 0.9942196531791907, 'f1': 0.9772727272727273}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'B-Drug': {'precision': 0.9907407407407407, 'recall': 0.9553571428571429, 'f1': 0.9727272727272727}, 'I-Drug': {'precision': 0.990990990990991, 'recall': 0.9821428571428571, 'f1': 0.9865470852017937}, 'B-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Finding': {'precision': 1.0, 'recall': 0.025, 'f1': 0.04878048780487806}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.302517294883728, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9653179190751445, 'recall': 0.9597701149425287, 'f1': 0.962536023054755}, 'I-ADR': {'precision': 0.9608938547486033, 'recall': 0.9942196531791907, 'f1': 0.9772727272727273}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'B-Drug': {'precision': 0.9907407407407407, 'recall': 0.9553571428571429, 'f1': 0.9727272727272727}, 'I-Drug': {'precision': 0.990990990990991, 'recall': 0.9821428571428571, 'f1': 0.9865470852017937}, 'B-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Finding': {'precision': 1.0, 'recall': 0.025, 'f1': 0.04878048780487806}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.4498057814601297, 'eval_micro_f1': 0.9056520198440822, 'eval_runtime': 93.8888, 'eval_samples_per_second': 2.13, 'eval_steps_per_second': 0.266, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30052ea2a2484735bd2ef9fea84cd0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9666666666666667, 'recall': 1.0, 'f1': 0.983050847457627}, 'I-ADR': {'precision': 0.9555555555555556, 'recall': 0.9942196531791907, 'f1': 0.9745042492917847}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.5333333333333333, 'recall': 0.25806451612903225, 'f1': 0.34782608695652173}, 'B-Drug': {'precision': 0.9910714285714286, 'recall': 0.9910714285714286, 'f1': 0.9910714285714286}, 'I-Drug': {'precision': 0.9910714285714286, 'recall': 0.9910714285714286, 'f1': 0.9910714285714286}, 'B-Finding': {'precision': 0.6666666666666666, 'recall': 0.17777777777777778, 'f1': 0.2807017543859649}, 'I-Finding': {'precision': 0.4583333333333333, 'recall': 0.275, 'f1': 0.34374999999999994}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26452377438545227, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9666666666666667, 'recall': 1.0, 'f1': 0.983050847457627}, 'I-ADR': {'precision': 0.9555555555555556, 'recall': 0.9942196531791907, 'f1': 0.9745042492917847}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.5333333333333333, 'recall': 0.25806451612903225, 'f1': 0.34782608695652173}, 'B-Drug': {'precision': 0.9910714285714286, 'recall': 0.9910714285714286, 'f1': 0.9910714285714286}, 'I-Drug': {'precision': 0.9910714285714286, 'recall': 0.9910714285714286, 'f1': 0.9910714285714286}, 'B-Finding': {'precision': 0.6666666666666666, 'recall': 0.17777777777777778, 'f1': 0.2807017543859649}, 'I-Finding': {'precision': 0.4583333333333333, 'recall': 0.275, 'f1': 0.34374999999999994}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.5374523450213414, 'eval_micro_f1': 0.9152639971651311, 'eval_runtime': 98.8973, 'eval_samples_per_second': 2.022, 'eval_steps_per_second': 0.253, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e6b9a6ba1946828d7c4cf697a1ba39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9613259668508287, 'recall': 1.0, 'f1': 0.9802816901408451}, 'I-ADR': {'precision': 0.9608938547486033, 'recall': 0.9942196531791907, 'f1': 0.9772727272727273}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.5294117647058824, 'recall': 0.2903225806451613, 'f1': 0.375}, 'B-Drug': {'precision': 0.9910714285714286, 'recall': 0.9910714285714286, 'f1': 0.9910714285714286}, 'I-Drug': {'precision': 1.0, 'recall': 0.9910714285714286, 'f1': 0.9955156950672646}, 'B-Finding': {'precision': 0.6190476190476191, 'recall': 0.28888888888888886, 'f1': 0.3939393939393939}, 'I-Finding': {'precision': 0.41935483870967744, 'recall': 0.325, 'f1': 0.36619718309859156}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2650657296180725, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9613259668508287, 'recall': 1.0, 'f1': 0.9802816901408451}, 'I-ADR': {'precision': 0.9608938547486033, 'recall': 0.9942196531791907, 'f1': 0.9772727272727273}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.5294117647058824, 'recall': 0.2903225806451613, 'f1': 0.375}, 'B-Drug': {'precision': 0.9910714285714286, 'recall': 0.9910714285714286, 'f1': 0.9910714285714286}, 'I-Drug': {'precision': 1.0, 'recall': 0.9910714285714286, 'f1': 0.9955156950672646}, 'B-Finding': {'precision': 0.6190476190476191, 'recall': 0.28888888888888886, 'f1': 0.3939393939393939}, 'I-Finding': {'precision': 0.41935483870967744, 'recall': 0.325, 'f1': 0.36619718309859156}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.5526616470991137, 'eval_micro_f1': 0.9162384833451452, 'eval_runtime': 93.7344, 'eval_samples_per_second': 2.134, 'eval_steps_per_second': 0.267, 'epoch': 3.0}\n",
      "{'train_runtime': 3419.5463, 'train_samples_per_second': 0.702, 'train_steps_per_second': 0.088, 'train_loss': 0.33488077799479166, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Bramve/bert-finetuned-ner/commit/cfdcd864f89584c212d773629d3daecc9d0f8c35', commit_message='Training complete', commit_description='', oid='cfdcd864f89584c212d773629d3daecc9d0f8c35', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################\n",
    "# ------------- Train Base-line Model ------------- #\n",
    "#####################################################\n",
    "\n",
    "# Set training arguments\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Push to hub (saving the train in HuggingFace)\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738eb12b57d647658c2c7fc109083154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78952f1be3f14565842d3cd7abfadba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2666c764ed3643c1b2f02389809d0a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68e2a0066d74d859bc9bbd4abf12627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31588e891bd464c902b100ae48aed48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9502762430939227, 'recall': 0.9608938547486033, 'f1': 0.9555555555555556}, 'I-ADR': {'precision': 0.9361702127659575, 'recall': 0.9943502824858758, 'f1': 0.9643835616438357}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.5, 'recall': 0.034482758620689655, 'f1': 0.06451612903225806}, 'B-Drug': {'precision': 0.9516129032258065, 'recall': 0.959349593495935, 'f1': 0.9554655870445343}, 'I-Drug': {'precision': 0.9918032786885246, 'recall': 0.983739837398374, 'f1': 0.9877551020408164}, 'B-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28709399700164795, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9502762430939227, 'recall': 0.9608938547486033, 'f1': 0.9555555555555556}, 'I-ADR': {'precision': 0.9361702127659575, 'recall': 0.9943502824858758, 'f1': 0.9643835616438357}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.5, 'recall': 0.034482758620689655, 'f1': 0.06451612903225806}, 'B-Drug': {'precision': 0.9516129032258065, 'recall': 0.959349593495935, 'f1': 0.9554655870445343}, 'I-Drug': {'precision': 0.9918032786885246, 'recall': 0.983739837398374, 'f1': 0.9877551020408164}, 'B-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.4479705395742727, 'eval_micro_f1': 0.9077959809539181, 'eval_runtime': 114.0973, 'eval_samples_per_second': 1.753, 'eval_steps_per_second': 0.219, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8286881c1dc4bf6b19f11822495dab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9516129032258065, 'recall': 0.9888268156424581, 'f1': 0.9698630136986301}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.5555555555555556, 'recall': 0.16129032258064516, 'f1': 0.25}, 'I-Disease': {'precision': 0.5454545454545454, 'recall': 0.41379310344827586, 'f1': 0.47058823529411764}, 'B-Drug': {'precision': 0.9834710743801653, 'recall': 0.967479674796748, 'f1': 0.9754098360655739}, 'I-Drug': {'precision': 0.991869918699187, 'recall': 0.991869918699187, 'f1': 0.991869918699187}, 'B-Finding': {'precision': 0.75, 'recall': 0.08333333333333333, 'f1': 0.15}, 'I-Finding': {'precision': 0.56, 'recall': 0.4117647058823529, 'f1': 0.4745762711864407}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24961765110492706, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9516129032258065, 'recall': 0.9888268156424581, 'f1': 0.9698630136986301}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.5555555555555556, 'recall': 0.16129032258064516, 'f1': 0.25}, 'I-Disease': {'precision': 0.5454545454545454, 'recall': 0.41379310344827586, 'f1': 0.47058823529411764}, 'B-Drug': {'precision': 0.9834710743801653, 'recall': 0.967479674796748, 'f1': 0.9754098360655739}, 'I-Drug': {'precision': 0.991869918699187, 'recall': 0.991869918699187, 'f1': 0.991869918699187}, 'B-Finding': {'precision': 0.75, 'recall': 0.08333333333333333, 'f1': 0.15}, 'I-Finding': {'precision': 0.56, 'recall': 0.4117647058823529, 'f1': 0.4745762711864407}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.5681218401797197, 'eval_micro_f1': 0.9159470583488015, 'eval_runtime': 109.8981, 'eval_samples_per_second': 1.82, 'eval_steps_per_second': 0.227, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6625739003a4a7f81f16fa470153bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9518716577540107, 'recall': 0.994413407821229, 'f1': 0.9726775956284153}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.6923076923076923, 'recall': 0.2903225806451613, 'f1': 0.4090909090909091}, 'I-Disease': {'precision': 0.5, 'recall': 0.4482758620689655, 'f1': 0.4727272727272727}, 'B-Drug': {'precision': 0.9836065573770492, 'recall': 0.975609756097561, 'f1': 0.9795918367346939}, 'I-Drug': {'precision': 0.991869918699187, 'recall': 0.991869918699187, 'f1': 0.991869918699187}, 'B-Finding': {'precision': 0.7777777777777778, 'recall': 0.19444444444444445, 'f1': 0.3111111111111111}, 'I-Finding': {'precision': 0.5555555555555556, 'recall': 0.4411764705882353, 'f1': 0.49180327868852464}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24656683206558228, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9518716577540107, 'recall': 0.994413407821229, 'f1': 0.9726775956284153}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.6923076923076923, 'recall': 0.2903225806451613, 'f1': 0.4090909090909091}, 'I-Disease': {'precision': 0.5, 'recall': 0.4482758620689655, 'f1': 0.4727272727272727}, 'B-Drug': {'precision': 0.9836065573770492, 'recall': 0.975609756097561, 'f1': 0.9795918367346939}, 'I-Drug': {'precision': 0.991869918699187, 'recall': 0.991869918699187, 'f1': 0.991869918699187}, 'B-Finding': {'precision': 0.7777777777777778, 'recall': 0.19444444444444445, 'f1': 0.3111111111111111}, 'I-Finding': {'precision': 0.5555555555555556, 'recall': 0.4411764705882353, 'f1': 0.49180327868852464}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.5996277172466438, 'eval_micro_f1': 0.9170769106609636, 'eval_runtime': 111.4636, 'eval_samples_per_second': 1.794, 'eval_steps_per_second': 0.224, 'epoch': 3.0}\n",
      "{'train_runtime': 3598.6275, 'train_samples_per_second': 0.667, 'train_steps_per_second': 0.083, 'train_loss': 0.3334148661295573, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Bramve/bert-finetuned-ner-CRF/commit/b47e8f043a6c8f0883328d4cb345e775a7de2012', commit_message='Training complete', commit_description='', oid='b47e8f043a6c8f0883328d4cb345e775a7de2012', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import get_full_repo_name, Repository, notebook_login, create_repo\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, pipeline, Trainer, TrainingArguments, get_scheduler\n",
    "from TorchCRF import CRF\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "label_names = ['O', 'B-ADR', 'I-ADR',\n",
    "               'B-Disease', 'I-Disease',\n",
    "               'B-Drug', 'I-Drug',\n",
    "               'B-Finding', 'I-Finding',\n",
    "               'B-Symptom', 'I-Symptom']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "tokenized_datasets = raw_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_data[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# add CRF layer to the model\n",
    "model.crf = CRF(num_labels=model.config.num_labels)\n",
    "model.config.num_labels\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner-CRF\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIlstm-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa87b0df48f4bfda3410e83d882bc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378a4f6cf50d42eab2ccf0fd2a7c3612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a0caa671264062af602041764650ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d63902609fc42a49a8c324904214a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc17b3af5cb42938298b43a392c0a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9510869565217391, 'recall': 0.9776536312849162, 'f1': 0.9641873278236915}, 'I-ADR': {'precision': 0.9408602150537635, 'recall': 0.9887005649717514, 'f1': 0.9641873278236914}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.625, 'recall': 0.1724137931034483, 'f1': 0.2702702702702703}, 'B-Drug': {'precision': 0.9754098360655737, 'recall': 0.967479674796748, 'f1': 0.9714285714285714}, 'I-Drug': {'precision': 1.0, 'recall': 0.991869918699187, 'f1': 0.9959183673469388}, 'B-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Finding': {'precision': 1.0, 'recall': 0.029411764705882353, 'f1': 0.05714285714285715}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2884244918823242, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9510869565217391, 'recall': 0.9776536312849162, 'f1': 0.9641873278236915}, 'I-ADR': {'precision': 0.9408602150537635, 'recall': 0.9887005649717514, 'f1': 0.9641873278236914}, 'B-Disease': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Disease': {'precision': 0.625, 'recall': 0.1724137931034483, 'f1': 0.2702702702702703}, 'B-Drug': {'precision': 0.9754098360655737, 'recall': 0.967479674796748, 'f1': 0.9714285714285714}, 'I-Drug': {'precision': 1.0, 'recall': 0.991869918699187, 'f1': 0.9959183673469388}, 'B-Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Finding': {'precision': 1.0, 'recall': 0.029411764705882353, 'f1': 0.05714285714285715}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.47483042925782004, 'eval_micro_f1': 0.9090872407392462, 'eval_runtime': 111.4475, 'eval_samples_per_second': 1.795, 'eval_steps_per_second': 0.224, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0614128f7d4a088ea10daf01fd34bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9516129032258065, 'recall': 0.9888268156424581, 'f1': 0.9698630136986301}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.7, 'recall': 0.22580645161290322, 'f1': 0.3414634146341463}, 'I-Disease': {'precision': 0.55, 'recall': 0.3793103448275862, 'f1': 0.4489795918367347}, 'B-Drug': {'precision': 0.9916666666666667, 'recall': 0.967479674796748, 'f1': 0.9794238683127573}, 'I-Drug': {'precision': 1.0, 'recall': 0.983739837398374, 'f1': 0.9918032786885246}, 'B-Finding': {'precision': 1.0, 'recall': 0.05555555555555555, 'f1': 0.10526315789473684}, 'I-Finding': {'precision': 0.6, 'recall': 0.35294117647058826, 'f1': 0.4444444444444445}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25115445256233215, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9516129032258065, 'recall': 0.9888268156424581, 'f1': 0.9698630136986301}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.7, 'recall': 0.22580645161290322, 'f1': 0.3414634146341463}, 'I-Disease': {'precision': 0.55, 'recall': 0.3793103448275862, 'f1': 0.4489795918367347}, 'B-Drug': {'precision': 0.9916666666666667, 'recall': 0.967479674796748, 'f1': 0.9794238683127573}, 'I-Drug': {'precision': 1.0, 'recall': 0.983739837398374, 'f1': 0.9918032786885246}, 'B-Finding': {'precision': 1.0, 'recall': 0.05555555555555555, 'f1': 0.10526315789473684}, 'I-Finding': {'precision': 0.6, 'recall': 0.35294117647058826, 'f1': 0.4444444444444445}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.5680248851402674, 'eval_micro_f1': 0.9165926882414656, 'eval_runtime': 111.5105, 'eval_samples_per_second': 1.794, 'eval_steps_per_second': 0.224, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a9831b38e64c44ab922eb8dd2d1c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Trainer is attempting to log a value of \"{'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9516129032258065, 'recall': 0.9888268156424581, 'f1': 0.9698630136986301}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.5882352941176471, 'recall': 0.3225806451612903, 'f1': 0.41666666666666663}, 'I-Disease': {'precision': 0.5, 'recall': 0.4482758620689655, 'f1': 0.4727272727272727}, 'B-Drug': {'precision': 0.9836065573770492, 'recall': 0.975609756097561, 'f1': 0.9795918367346939}, 'I-Drug': {'precision': 0.991869918699187, 'recall': 0.991869918699187, 'f1': 0.991869918699187}, 'B-Finding': {'precision': 0.875, 'recall': 0.19444444444444445, 'f1': 0.3181818181818182}, 'I-Finding': {'precision': 0.7368421052631579, 'recall': 0.4117647058823529, 'f1': 0.5283018867924528}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}\" of type <class 'dict'> for key \"eval/entity_metrics\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24407970905303955, 'eval_entity_metrics': {'O': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'B-ADR': {'precision': 0.9516129032258065, 'recall': 0.9888268156424581, 'f1': 0.9698630136986301}, 'I-ADR': {'precision': 0.9411764705882353, 'recall': 0.9943502824858758, 'f1': 0.9670329670329672}, 'B-Disease': {'precision': 0.5882352941176471, 'recall': 0.3225806451612903, 'f1': 0.41666666666666663}, 'I-Disease': {'precision': 0.5, 'recall': 0.4482758620689655, 'f1': 0.4727272727272727}, 'B-Drug': {'precision': 0.9836065573770492, 'recall': 0.975609756097561, 'f1': 0.9795918367346939}, 'I-Drug': {'precision': 0.991869918699187, 'recall': 0.991869918699187, 'f1': 0.991869918699187}, 'B-Finding': {'precision': 0.875, 'recall': 0.19444444444444445, 'f1': 0.3181818181818182}, 'I-Finding': {'precision': 0.7368421052631579, 'recall': 0.4117647058823529, 'f1': 0.5283018867924528}, 'B-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}, 'I-Symptom': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}}, 'eval_macro_f1': 0.6040213982303353, 'eval_micro_f1': 0.9195787264950367, 'eval_runtime': 110.9588, 'eval_samples_per_second': 1.802, 'eval_steps_per_second': 0.225, 'epoch': 3.0}\n",
      "{'train_runtime': 3579.7884, 'train_samples_per_second': 0.67, 'train_steps_per_second': 0.084, 'train_loss': 0.33512133280436196, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Bramve/bert-finetuned-ner-bilstm/commit/ec06e5eb7480327fc14d02f4ee82086972b44ff7', commit_message='Training complete', commit_description='', oid='ec06e5eb7480327fc14d02f4ee82086972b44ff7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "label_names = ['O', 'B-ADR', 'I-ADR',\n",
    "               'B-Disease', 'I-Disease',\n",
    "               'B-Drug', 'I-Drug',\n",
    "               'B-Finding', 'I-Finding',\n",
    "               'B-Symptom', 'I-Symptom']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "tokenized_datasets = raw_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_data[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "#add bilstm layer to the model\n",
    "model.bilstm = torch.nn.LSTM(model.config.hidden_size, model.config.hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "model.config.num_labels\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner-bilstm\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing and Training with Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'bert-finetuned-ner-accelerate/'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:164\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'bert-finetuned-ner-accelerate/'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#####################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ------------ Set-up with Accelerator ------------ #\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#####################################################\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Replace this with your own checkpoint\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-finetuned-ner-accelerate/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m token_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Get dataloader for train and evaluation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     13\u001b[0m     tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     14\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     16\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\pipelines\\__init__.py:747\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    748\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    749\u001b[0m         CONFIG_NAME,\n\u001b[0;32m    750\u001b[0m         _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    751\u001b[0m         _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    752\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bramv\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\hub.py:454\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: 'bert-finetuned-ner-accelerate/'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# ------------ Set-up with Accelerator ------------ #\n",
    "#####################################################\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = 'bert-finetuned-ner-accelerate/'\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Get dataloader for train and evaluation\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "# Get model from the new pretrained checkpoint\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Set optimizer AdamW in this case\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Prepare\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "# Set accelerator train parameters\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# --------------- HuggingFace Setup --------------- #\n",
    "#####################################################\n",
    "\n",
    "# Set model and repository name\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "print(f'Repo name: {repo_name}')\n",
    "\n",
    "# Create repository\n",
    "create_repo(\"Gorgoura/bert-finetuned-ner-accelerate\", repo_type=\"model\")\n",
    "\n",
    "# Set ouput directory and make the repository\n",
    "output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_training_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#####################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ----------- Training with Accelerator ----------- #\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#####################################################\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_training_steps\u001b[49m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_training_steps' is not defined"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# ----------- Training with Accelerator ----------- #\n",
    "#####################################################\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# ------------ Evaluating the training ------------ #\n",
    "#####################################################\n",
    "\n",
    "# Initializing the trainer again, to run the evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    # eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results and the checkpoint used\n",
    "print (f\"checkpoint: {model_checkpoint}\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"{key}: {value:.3f}\")\n",
    "\n",
    "# Print all entity metrics\n",
    "entity_metrics = test_results[\"eval_entity_metrics\"]\n",
    "macro_f1 = test_results[\"eval_macro_f1\"]\n",
    "micro_f1 = test_results[\"eval_micro_f1\"]\n",
    "\n",
    "# Display the evaluation results\n",
    "print(\"Entity Metrics:\")\n",
    "for entity, metrics in entity_metrics.items():\n",
    "    print(f\"{entity}: Precision={metrics['precision']}, Recall={metrics['recall']}, F1={metrics['f1']}\")\n",
    "\n",
    "print(f\"Macro-Averaged F1: {macro_f1}\")\n",
    "print(f\"Micro-Averaged F1: {micro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
