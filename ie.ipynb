{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import get_full_repo_name, Repository, notebook_login\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, pipeline, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Step\n",
    "Convert data into with aligned labels and tokens to perform the NER in IOB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "TEXT_FOLDER = './data/CADEC.v2/cadec/text/'\n",
    "OG_ANN_FOLDER = './data/CADEC.v2/cadec/original/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read text from a .txt file\n",
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to read annotations from a .ann file\n",
    "def read_annotations(file_path):\n",
    "    annotations = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            # Get only the important information\n",
    "            information = parts[1].split()\n",
    "\n",
    "            # Get values from info\n",
    "            label = information[0]\n",
    "            # Semicolon problem not solved\n",
    "            start_values = [value for value in information[1].split(';')]\n",
    "            end_values = [value for value in information[2].split(';')]\n",
    "\n",
    "            for start, end in zip(start_values, end_values):\n",
    "                annotations.append({\"start\": int(start), \"end\": int(end), \"label\": label})\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Update tags based on the annotations\n",
    "def annotate_text(doc, annotations):\n",
    "    tags = [\"O\"] * len(doc)\n",
    "    for annotation in annotations:\n",
    "        start, end, label = annotation[\"start\"], annotation[\"end\"], annotation[\"label\"]\n",
    "        start_token = None\n",
    "        for i, token in enumerate(doc):\n",
    "            if start_token is None and token.idx >= start:\n",
    "                start_token = i\n",
    "            if token.idx + len(token) >= end:\n",
    "                for j in range(start_token, i + 1):\n",
    "                    if j == start_token:\n",
    "                        tags[j] = f\"B-{label}\"\n",
    "                    else:\n",
    "                        tags[j] = f\"I-{label}\"\n",
    "                break\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "TEXT_FOLDER = './data/CADEC.v2/cadec/text/'\n",
    "OG_ANN_FOLDER = './data/CADEC.v2/cadec/original/'\n",
    "\n",
    "# Function to read text from a .txt file\n",
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to read annotations from a .ann file\n",
    "def read_annotations(file_path):\n",
    "    annotations = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            # Get only the important information\n",
    "            information = parts[1].split()\n",
    "\n",
    "            # Get values from info\n",
    "            label = information[0]\n",
    "            # Semicolon problem not solved\n",
    "            start_values = [value for value in information[1].split(';')]\n",
    "            end_values = [value for value in information[2].split(';')]\n",
    "\n",
    "            for start, end in zip(start_values, end_values):\n",
    "                annotations.append({\"start\": int(start), \"end\": int(end), \"label\": label})\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Update tags based on the annotations\n",
    "def annotate_text(doc, annotations):\n",
    "    tags = [\"O\"] * len(doc)\n",
    "    for annotation in annotations:\n",
    "        start, end, label = annotation[\"start\"], annotation[\"end\"], annotation[\"label\"]\n",
    "        start_token = None\n",
    "        for i, token in enumerate(doc):\n",
    "            if start_token is None and token.idx >= start:\n",
    "                start_token = i\n",
    "            if token.idx + len(token) >= end:\n",
    "                print(start_token)\n",
    "                for j in range(start_token, i + 1):\n",
    "                    if j == start_token:\n",
    "                        tags[j] = f\"B-{label}\"\n",
    "                    else:\n",
    "                        tags[j] = f\"I-{label}\"\n",
    "                break\n",
    "    return tags\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get a list of all text files in the folder\n",
    "text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "\n",
    "# Initialize lists to store data\n",
    "data = []\n",
    "\n",
    "# Loop through each text file\n",
    "for text_file in text_files:\n",
    "    print(text_file)\n",
    "    # Build file paths\n",
    "    txt_file_path = os.path.join(TEXT_FOLDER, text_file)\n",
    "    ann_file_path = os.path.join(OG_ANN_FOLDER, text_file.replace(\".txt\", \".ann\"))\n",
    "\n",
    "    # Read text from the .txt file\n",
    "    text = read_text(txt_file_path)\n",
    "\n",
    "    # Read annotations from the .ann file\n",
    "    annotations = read_annotations(ann_file_path)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Perform the annotation loop\n",
    "    tags_array = annotate_text(doc, annotations)\n",
    "\n",
    "    # Create a word by word array\n",
    "    words_array = [token.text for token in doc]\n",
    "\n",
    "    # Store the data for this document\n",
    "    data.append({\"Words\": words_array, \"Tags\": tags_array})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Sample text and annotations\n",
    "# text = (\n",
    "#     \"I feel a bit drowsy & have a little blurred vision, so far no gastric problems. \"\n",
    "#     \"I've been on Arthrotec 50 for over 10 years on and off, only taking it when I needed it. \"\n",
    "#     \"Due to my arthritis getting progressively worse, to the point where I am in tears with the agony, \"\n",
    "#     \"gp's started me on 75 twice a day and I have to take it. \"\n",
    "#     \"every day for the next month to see how I get on, here goes. \"\n",
    "#     \"So far its been very good, pains almost gone, but I feel a bit weird, didn't have that when on 50.\"\n",
    "# )\n",
    "\n",
    "# annotations = [\n",
    "#     {\"start\": 9, \"end\": 19, \"label\": \"ADR\"},  # Drowsy\n",
    "#     {\"start\": 29, \"end\": 50, \"label\": \"ADR\"},  # Blurred Vision\n",
    "#     {\"start\": 93, \"end\": 102, \"label\": \"Drug\"},  # Arthrotec\n",
    "#     {\"start\": 179, \"end\": 188, \"label\": \"Disease\"},  # arthritis\n",
    "#     {\"start\": 260, \"end\": 265, \"label\": \"Symptom\"},  # agony\n",
    "#     {\"start\": 62, \"end\": 78, \"label\": \"ADR\"},  # gastric problems\n",
    "#     {\"start\": 412, \"end\": 417, \"label\": \"Symptom\"},  # pains\n",
    "#     {\"start\": 437, \"end\": 453, \"label\": \"ADR\"},  # feel a bit weird\n",
    "# ]\n",
    "\n",
    "# # Process the text with spaCy\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Update annotations based on spaCy entities\n",
    "# for ent in doc.ents:\n",
    "#     if ent.label_ in [\"Drug\", \"ADR\", \"Disease\", \"Symptom\"]:\n",
    "#         start, end, label = ent.start_char, ent.end_char, ent.label_\n",
    "#         annotations.append({\"start\": start, \"end\": end, \"label\": label})\n",
    "\n",
    "# # Initialize IOB tags for each token\n",
    "# tags = [\"O\"] * len(doc)\n",
    "\n",
    "# # Update tags based on annotations\n",
    "# for annotation in annotations:\n",
    "#     start, end, label = annotation[\"start\"], annotation[\"end\"], annotation[\"label\"]\n",
    "#     start_token = None\n",
    "#     for i, token in enumerate(doc):\n",
    "#         if start_token is None and token.idx >= start:\n",
    "#             start_token = i\n",
    "#         if token.idx + len(token) >= end:\n",
    "#             for j in range(start_token, i + 1):\n",
    "#                 if j == start_token:\n",
    "#                     tags[j] = f\"B-{label}\"\n",
    "#                 else:\n",
    "#                     tags[j] = f\"I-{label}\"\n",
    "#             break\n",
    "\n",
    "# # Convert the tags to a string array\n",
    "# tags_array = [f\"{tag}\" for tag in tags]\n",
    "\n",
    "# # Create a word by word array\n",
    "# words_array = [token.text for token in doc]\n",
    "\n",
    "# # Print the result\n",
    "# print(\"Words array:\", words_array)\n",
    "# print(\"Tags array:\", tags_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
